<!DOCTYPE html>
<meta charset="utf-8">

<html>

<style type="text/css">
body {
	font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	font-weight: 300;
	font-size: 17px;
	margin-left: auto;
	margin-right: auto;
	width: 980px;
}
h1 {
	font-weight:300;
	line-height: 1.15em;
}

h2 {
	font-size: 1.75em;
}
a:link,a:visited {
	color: #1367a7;
	text-decoration: none;
}
a:hover {
	color: #208799;
}
h1, h2, h3 {
	text-align: center;
}
h1 {
	font-size: 40px;
	font-weight: 500;
}
h2, h3 {
	font-weight: 400;
	margin: 16px 0px 4px 0px;
}
.paper-title {
	padding: 16px 0px 16px 0px;
}
section {
	margin: 32px 0px 32px 0px;
	text-align: justify;
	clear: both;
}
.col-5 {
	 width: 20%;
	 float: left;
}
.col-4 {
	 width: 25%;
	 float: left;
}
.col-2 {
	 width: 50%;
	 float: left;
}
.row, .author-row, .affil-row {
	 overflow: auto;
}
.author-row, .affil-row {
	font-size: 20px;
}
.row {
	margin: 16px 0px 16px 0px;
}
.authors {
	font-size: 18px;
}
.affil-row {
	margin-top: 16px;
}
.teaser {
	max-width: 100%;
}
.text-center {
	text-align: center;
}
.screenshot {
	width: 256px;
	border: 1px solid #ddd;
}
.screenshot-el {
	margin-bottom: 16px;
}
hr {
	height: 1px;
	border: 0;
	border-top: 1px solid #ddd;
	margin: 0;
}
.material-icons {
	vertical-align: -6px;
}
p {
	line-height: 1.25em;
}
.caption_justify {
	font-size: 16px;
	/*font-style: italic;*/
	color: #666;
	text-align: justify;
	margin-top: 0px;
	margin-bottom: 64px;
}
.caption {
	font-size: 16px;
	/*font-style: italic;*/
	color: #666;
	text-align: center;
	margin-top: 8px;
	margin-bottom: 64px;
}
.caption_inline {
	font-size: 16px;
	/*font-style: italic;*/
	color: #666;
	text-align: center;
	margin-top: 8px;
	margin-bottom: 0px;
}
.caption_bold {
	font-size: 16px;
	/*font-style: italic;*/
	color: #666;
	text-align: center;
	margin-top: 0px;
	margin-bottom: 0px;
	font-weight: bold;
}
video {
	display: block;
	margin: auto;
}
figure {
	display: block;
	margin: auto;
	margin-top: 10px;
	margin-bottom: 10px;
}
figure {
	display: block;
	margin: auto;
	margin-top: 10px;
	margin-bottom: 10px;
}
#bibtex pre {
	font-size: 14px;
	background-color: #eee;
	padding: 16px;
}
.blue {
	color: #2c82c9;
	font-weight: bold;
}
.orange {
	color: #d35400;
	font-weight: bold;
}
.flex-row {
	display: flex;
	flex-flow: row wrap;
	justify-content: space-around;
	padding: 0;
	margin: 0;
	list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;

  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
	display: flex;
	justify-content: center;
	margin: 16px 0px;
}
.paper-btn:hover {
	opacity: 0.85;
}
.container {
	margin-left: auto;
	margin-right: auto;
	padding-left: 16px;
	padding-right: 16px;
}
.venue {
	color: #1367a7;
}
</style>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
	<title>Real-time X-ray coherent diffraction microscopy using deep learning at the edge</title>
	<meta property="og:description" content="Real-time X-ray coherent diffraction microscopy using deep learning at the edge"/>
	<link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

	
</head>

<body>
<div class="container">
	<div class="paper-title">
		<h1>Real-time X-ray coherent diffraction microscopy using deep learning at the edge</h1>
	</div>

	<div id="authors">
		<div class="author-row">
			<div class="col-4 text-center"><a href="https://www.anl.gov/profile/anakha-v-babu">Anakha V Babu</a></div>
			<div class="col-4 text-center"><a href="https://www.anl.gov/profile/tao-zhou">Zhou Tao</a></div>
			<div class="col-4 text-center"><a href="https://research.nvidia.com/person/christoph-schied">Saugat Kandel</a></div>
			<div class="col-4 text-center"><a href="">Yi Jiang</a></div>
			<div class="col-4 text-center"><a href="">Yi Jiang</a></div>
			<div class="col-4 text-center"><a href="">Yi Jiang</a></div>
			<div class="col-4 text-center"><a href="">Yi Jiang</a></div>
			<div class="col-4 text-center"><a href="">Yi Jiang</a></div>
			<div class="col-4 text-center"><a href="">Yi Jiang</a></div>
			<div class="col-4 text-center"><a href="">Yi Jiang</a></div>
		</div>

		
	</section>

	<section id="news">
		<h2>News</h2>
		<hr>
		<div class="row">
			<div><span class="material-icons"> description </span> [May 3rd 2022] Paper accepted to <a href="https://s2022.siggraph.org">ACM Transactions on Graphics (SIGGRAPH 2022)</a>.</div>
			<div><span class="material-icons"> description </span> [Jan 19th 2022] Paper released on <a href="https://arxiv.org/abs/2201.05989">arXiv</a>.</div>
			<div><span class="material-icons"> integration_instructions </span> [Jan 14th 2022] Code released on <a href="https://github.com/NVlabs/instant-ngp">GitHub</a>.</div>
		</div>
	</section>

	<section id="abstract"/>
		<h2>Abstract</h2>
		<hr>
		<p>
			Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations. A small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920x1080.
		</p>
	</section>



	
	





		<h3>Signed Distance Function</h3>
		<hr>
		<figure style="width: 100.0%;">
			<video class="centered" width="100.0%" autoplay muted loop playsinline>
				<source src="assets/sdf_grid_lq.mp4" type="video/mp4">
				Your browser does not support the video tag.
			</video>
		</figure>
		<p class="caption_justify">
			Real-time training progress on various SDF datsets. Training data is generated on the fly from the ground-truth mesh using the <a href="https://developer.nvidia.com/optix">NVIDIA OptiX raytracing framework</a>.
			Bearded Man model &copy;Oliver Laric (<a href="https://creativecommons.org/licenses/by-nc-sa/3.0/">CC BY-NC-SA 3.0</a>)
		</p>

		<h3>Neural Radiance Cache</h3>
		<hr>
		<figure>
			<video class="centered" width="100%" autoplay muted loop playsinline>
				<source src="assets/nrc_new_vs_old.mp4" type="video/mp4">
				Your browser does not support the video tag.
			</video>
		</figure>
		<p class="caption_justify">
			Direct visualization of a <em>neural radiance cache</em>, in which the network predicts outgoing radiance at the first non-specular vertex of each pixel's path, and is trained on-line from rays generated by a real-time pathtracer. On the left, we show results using the triangle wave encoding of <a href="https://research.nvidia.com/publication/2021-06_Real-time-Neural-Radiance">[MÃ¼ller et al. 2021]</a>; on the right, the new multiresolution hash encoding allows the network to learn much sharper details, for example in the shadow regions.
		</p>


	</section>

	<section id="bibtex">
		<h2>Citation</h2>
		<hr>
		<pre><code>@article{mueller2022instant,
    author = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
    title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
    journal = {ACM Trans. Graph.},
    issue_date = {July 2022},
    volume = {41},
    number = {4},
    month = jul,
    year = {2022},
    pages = {102:1--102:15},
    articleno = {102},
    numpages = {15},
    url = {https://doi.org/10.1145/3528223.3530127},
    doi = {10.1145/3528223.3530127},
    publisher = {ACM},
    address = {New York, NY, USA}
}
</code></pre>
	</section>

	<section id="acknowledgements">
		<h2>Acknowledgements</h2>
		<hr>
		<div class="row">
			<p>
			We would like to thank
			<a href="https://anjulpatney.com/">Anjul Patney</a>,
			<a href="https://luebke.us/">David Luebke</a>,
			<a href="https://research.nvidia.com/person/jacob-munkberg">Jacob Munkberg</a>,
			<a href="http://granskog.xyz/">Jonathan Granskog</a>,
			<a href="https://www.cs.mcgill.ca/~jtremb59/">Jonathan Tremblay</a>,
			<a href="https://luminohope.org/">Koki Nagano</a>,
			<a href="https://research.nvidia.com/person/marco-salvi">Marco Salvi</a>,
			<a href="https://research.nvidia.com/person/nikolaus-binder">Nikolaus Binder</a>,
			<a href="https://www.cs.toronto.edu/~jlucas/">James Lucas</a>, and
			<a href="https://tovacinni.github.io">Towaki Takikawa</a>
			for proof-reading, feedback, profound discussions, and early testing.
			We also thank <a href="https://joeylitalien.github.io/">Joey Litalien</a> for providing us with the framework for this website.
			<br/>
			<br/>
			<em>Girl With a Pearl Earring</em> renovation &copy;Koorosh Orooj (<a href="http://profoundism.com/free_licenses.html">CC BY-SA 4.0</a>)
			<br/>
			<em>Tokyo</em> gigapixel image &copy;Trevor Dobson (<a href="https://creativecommons.org/licenses/by-nc-nd/2.0/">CC BY-NC-ND 2.0</a>)
			<br/>
			<em>Lucy</em> model from the <a href="http://graphics.stanford.edu/data/3Dscanrep/">Stanford 3D scan repository</a>
			<br/>
			<em>Factory robot</em> dataset by Arman Toorians and Saurabh Jain
			<br/>
			<em>Disney Cloud</em> model &copy;Walt Disney Animation Studios (<a href="https://media.disneyanimation.com/uploads/production/data_set_asset/6/asset/License_Cloud.pdf">CC BY-SA 3.0</a>)
			<br/>
			<em>Bearded Man</em> model &copy;Oliver Laric (<a href="https://creativecommons.org/licenses/by-nc-sa/3.0/">CC BY-NC-SA 3.0</a>)
			</p>
		</div>
	</section>
</div>
</body>
</html>
